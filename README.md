# Demo data science template

[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/charliermarsh/ruff)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)
[![CI](https://github.com/JoseRZapata/demo-data-science-template/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/JoseRZapata/demo-data-science-template/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/JoseRZapata/demo-data-science-template/graph/badge.svg?token=PpCcK9jKy9)](https://codecov.io/gh/JoseRZapata/demo-data-science-template)
---

This demo of a data science project is created using the template from [@JoseRZapata]'s [data science project template] which have all the necessary tools for experiment, development, testing, and deployment data science From notebooks to production.

> [!WARNING]
> ðŸš§ Work in progress ðŸš§, This is a demo project, It is only for educational purposes.

## Install the proyect

In Linux / MAC OS

1. Install [UV](https://docs.astral.sh/uv/) to manage python dependencies and environments
2. clone repository
3. In the root of the proyect only run: `uv sync` to create the environment
4. activate the environment `source .venv/bin/activate`

Enjoy!!

## FastAPI Demo

To run the fastapi demo, you can run the following command:

`fastapi dev src/inference/model_deploy.py`

Then, you can access the API Documentation at <http://127.0.0.1:8000/docs>

## Streamlit Demo

To run the streamlit demo, you can run the following command:

`streamlit run notebooks/7-deploy/titanic-streamlit.py`

## Run Training pipeline

To run the simple training, you can run the following command in the environment:

`python src/pipelines/simple_train_pipeline.py`

Created Trained model will be in: `models/first_basic_model.joblib`

## ðŸ—ƒï¸ Project structure

- [Data structure]
- [Pipelines based on Feature/Training/Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)

```bash
.
â”œâ”€â”€ codecov.yml                         # configuration for codecov
â”œâ”€â”€ .code_quality
â”‚Â Â  â”œâ”€â”€ mypy.ini                        # mypy configuration
â”‚Â Â  â””â”€â”€ ruff.toml                       # ruff configuration
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ 01_raw                          # raw immutable data
â”‚Â Â  â”œâ”€â”€ 02_intermediate                 # typed data
â”‚Â Â  â”œâ”€â”€ 03_primary                      # domain model data
â”‚Â Â  â”œâ”€â”€ 04_feature                      # model features
â”‚Â Â  â”œâ”€â”€ 05_model_input                  # often called 'master tables'
â”‚Â Â  â”œâ”€â”€ 06_models                       # serialized models
â”‚Â Â  â”œâ”€â”€ 07_model_output                 # data generated by model runs
â”‚Â Â  â”œâ”€â”€ 08_reporting                    # reports, results, etc
â”‚Â Â  â””â”€â”€ README.md                       # description of the data structure
â”œâ”€â”€ docs                                # documentation for your project
â”œâ”€â”€ .editorconfig                       # editor configuration
â”œâ”€â”€ .github                             # github configuration
â”‚Â Â  â”œâ”€â”€ dependabot.md                   # github action to update dependencies
â”‚Â Â  â”œâ”€â”€ pull_request_template.md        # template for pull requests
â”‚Â Â  â””â”€â”€ workflows                       # github actions workflows
â”‚Â Â      â”œâ”€â”€ ci.yml                      # run continuous integration (tests, pre-commit, etc.)
â”‚Â Â      â”œâ”€â”€ dependency_review.yml       # review dependencies
â”‚Â Â      â”œâ”€â”€ docs.yml                    # build documentation (mkdocs)
â”‚Â Â      â””â”€â”€ pre-commit_autoupdate.yml   # update pre-commit hooks
â”œâ”€â”€ .gitignore                          # files to ignore in git
â”œâ”€â”€ Makefile                            # useful commands to setup environment, run tests, etc.
â”œâ”€â”€ models                              # store final models
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ 1-data                          # data extraction and cleaning
â”‚Â Â  â”œâ”€â”€ 2-exploration                   # exploratory data analysis (EDA)
â”‚Â Â  â”œâ”€â”€ 3-analysis                      # Statistical analysis, hypothesis testing.
â”‚Â Â  â”œâ”€â”€ 4-feat_eng                      # feature engineering (creation, selection, and transformation.)
â”‚Â Â  â”œâ”€â”€ 5-models                        # model training, evaluation, and hyperparameter tuning.
â”‚Â Â  â”œâ”€â”€ 6-interpretation                # model interpretation
â”‚Â Â  â”œâ”€â”€ 7-deploy                        # model packaging, deployment strategies.
â”‚Â Â  â”œâ”€â”€ 8-reports                       # story telling, summaries and analysis conclusions.
â”‚Â Â  â”œâ”€â”€ notebook_template.ipynb         # template for notebooks
â”‚Â Â  â””â”€â”€ README.md                       # information about the notebooks
â”œâ”€â”€ .pre-commit-config.yaml             # configuration for pre-commit hooks
â”œâ”€â”€ pyproject.toml                      # dependencies for the python project
â”œâ”€â”€ README.md                           # description of your project
â”œâ”€â”€ src                                 # source code for use in this project
â”‚   â”œâ”€â”€ README.md                       # description of src structure
â”‚   â”œâ”€â”€ tmp_mock.py                     # example python file
â”‚   â”œâ”€â”€ data                            # data extraction, validation, processing, transformation
â”‚   â”œâ”€â”€ model                           # model training, evaluation, validation, export
â”‚   â”œâ”€â”€ inference                       # model prediction, serving, monitoring
â”‚   â””â”€â”€ pipelines                       # orchestration of pipelines
â”‚       â”œâ”€â”€ feature_pipeline            # transforms raw data into features and labels
â”‚       â”œâ”€â”€ training_pipeline           # transforms features and labels into a model
â”‚       â””â”€â”€ inference_pipeline          # takes features and a trained model for predictions
â”œâ”€â”€ tests                               # test code for your project
â”‚   â”œâ”€â”€ test_mock.py                    # example test file
â”‚   â”œâ”€â”€ data                            # tests for data module
â”‚   â”œâ”€â”€ model                           # tests for model module
â”‚   â”œâ”€â”€ inference                       # tests for inference module
â”‚   â””â”€â”€ pipelines                       # tests for pipelines module
â””â”€â”€ .vscode                             # vscode configuration
    â”œâ”€â”€ extensions.json                 # list of recommended extensions
    â”œâ”€â”€ launch.json                     # vscode launch configuration
    â””â”€â”€ settings.json                   # vscode settings
```

## Data Science Code structure

### Orchestrated experiment

```mermaid
flowchart TD
  subgraph input [ETL]
    %%nodes
    A1[(Data web)]
    B[Process_etl]
    BB1{{Data integrity}}
    BB2{{Data Validation}}
    Dcheck[(Data Checked)]

    %%links
    A1 ==>B
    B ==> BB1 ==> BB2 ==> Dcheck[(Data Checked)]
  end

  subgraph split [Train / Test data split]
    %%nodes
    C[Split - Train /Test]
    C1[(Train)]
    C2[(Test)]
    CC{{Train / Test Validation}}

    %%links
    Dcheck ==> C
    C --> |data test|C2
    C --> |data train|C1
    C2 & C1 --> CC
  end

  subgraph train_feature [Train Feature Engineering]
    %%nodes
    D[<b>Pre - process Train</b> <br> Not needed in test <br> Ex:  Remove outliers, Duplicated, Drops]


    subgraph feature [Feature Engineering pipeline <br> for use in train and test]
      style feature fill:grey,stroke:#333,stroke-width:2px
      %%nodes
      E[<b>Initial Processing</b> <br> Ex: Casting, New columns, Replace empty values for NaN]
      F{Split <br> Data Type}
      G1[Transformation <br> Numerics <br> <s>No Drops</s>]
      G2[Transformation <br> Categoric <br> <s>No Drops</s>]
      G3[Transformation <br> Booleans <br> <s>No Drops</s>]
      G4[Transformation <br> Dates <br> <s>No Drops</s>]
      G5[Transformation <br> Strings <br> <s>No Drops</s>]
      H[<b>Final Processing</b> <br> Final Pipeline <br> ColumnTranformer <br> and last transforms]
      TRfit[Train Transformer]
      TRdb[(Transformer <br> Pipeline)]
      %%links
      E -.-> F
      F -.->|Numeric|G1
      F -.->|Categoric|G2
      F -.->|Bool|G3
      F -.->|Dates|G4
      F -.->|Strings|G5
      G1 & G2 & G3 & G4 & G5 -.-> H

      H -.-> |objeto pipeline|TRfit
      TRfit -.-> |objeto pipeline|TRdb
    end

    %%nodes
    I[<b>Post - Processing Train</b> <br> Ej: Data Balance - smote, Drop duplicates <br> Not needed in test]

    %%links
    C1 --->D
    D --> |X - data train <br> pre-processed|E
    D --> |X - data train <br> pre-processed|TRfit
    TRfit --> |X - data train <br> transformed|I
    D --> |Y - data train <br> pre-processed|I

  end

  subgraph mod[Modeling]

    %%nodes
    J[Modeling]
    Modeldb[(Train Model <br> candidate)]

    %%links
    I ---> |X - data train <br> post-processed|J
    I --> |Y - data train <br> post-processed|J
    J -.-> |Model Object| Modeldb

  end

  subgraph pred [Prediction]
    %%nodes
    TRtest[Transformation <br> X - Data test]
    Pred_test[Prediction test]
    Pred_train[Prediction train]
    Pred_db[(Predictions)]

    %%links
    C2 --> |X - data test|TRtest
    TRdb -.->TRtest
    TRtest --> |X - data test <br> transformed|Pred_test
    C2 --> |Y - data test|Pred_test
    I --> |X - data train <br> post-processed|Pred_train
    I --> |Y - data train <br> post-processed|Pred_train

    Modeldb -.-> |model|Pred_train --> Pred_db
    Modeldb -.-> |model|Pred_test --> Pred_db
  end

  subgraph eval [Evaluation]
    %%nodes
    Modelcheck{{Model validation}}
    M[Eval]
    N[(Score)]

    %%links

    I  --> |X data train <br> post-processed|Modelcheck
    I  --> |Y data train <br> post-processed|Modelcheck
    TRtest --> |X - data test <br> transformed|Modelcheck
    C2 --> |Y - data test|Modelcheck
    Modeldb -....-> |model|Modelcheck
    Pred_db --> M
    M -.->N
  end

  %%links


  Modelcheck -..->  pass{Pass ?}
  pass -.-> |no|no((Alert!))
  pass -.-> |yes|si(Execute modeling <br> with all data):::Passclass


  %% Definine link styles
  linkStyle default stroke:blue

  linkStyle 8,10,12,33,35,42,45,46 stroke:orange
  linkStyle 29,31,38,44 stroke:deepskyblue
  linkStyle 36,46 stroke:gold

  %% Styling the title subgraph
  classDef Title stroke-width:0, color:#f66,  font-weight:bold, font-size: 24px;

  class input,train_feature,feature,pred,mod,eval Title


  %% Definine node styles
  classDef Objclass fill:#329cc1;
  classDef Checkclass fill:#EC5800;
  classDef Alertclass fill:#FF0000;
  classDef Passclass fill:#00CC88;

  %% Assigning styles to nodes
  class C1,C2,Dcheck,TRdb,Modeldb,Pred_db,N Objclass;
  class BB1,BB2,CC,Modelcheck Checkclass;
  class no Alertclass;
  class si Passclass;
```

### Deployment

```mermaid
flowchart TD
  orch_exp[Orchestrated Experiment] -.-> Modelcheck
  Modelcheck{{Model validation}}:::Checkclass -.-> |si| input
  Modelcheck -.-> |no|stop((Alert! <br> Stop)):::Alertclass
  subgraph input [ETL]
   Dcheck[(Data Checked)]:::Objclass
  end
  Dcheck ==>  D[<b>Pre - processing</b>]

  subgraph train_feature [Train Feature Engineering]
    %%nodes
    D[<b>Pre - processing Train</b> <br> Not needed in test <br> Ej: Drop outliers, Duplicates, Drops]


    subgraph feature [Feature Engineering pipeline <br> for use in train and test]
      style feature fill:grey,stroke:#333,stroke-width:2px
      %%nodes
      E[<b>Initial Processing </b> <br> Ej: Casting, New columns, Replace empty values for NaN]
      F{Split <br> Data Type}
      G1[Transformation <br> Numerics <br> <s>No Drops</s>]
      G2[Transformation <br> Categoric <br> <s>No Drops</s>]
      G3[Transformation <br> Booleans <br> <s>No Drops</s>]
      G4[Transformation <br> Dates <br> <s>No Drops</s>]
      G5[Transformation <br> Strings <br> <s>No Drops</s>]
      H[<b>Processing Final</b> <br> Final Pipeline <br> ColumnTranformer <br>  and final transforms]
      TRfit[Train Transformer]
      %%links
      E -.-> F
      F -.->|Numeric|G1
      F -.->|Categoric|G2
      F -.->|Bool|G3
      F -.->|Dates|G4
      F -.->|Strings|G5
      G1 & G2 & G3 & G4 & G5 -.-> H

      H -.-> |objeto pipeline|TRfit
    end

    %%nodes
    I[<b>Post - Processing Train</b> <br> Ej: Data Balance - smote, dorp duplicates <br> Not needed in test]

    %%links

    D --> |X - data train <br> pre-processed|E
    D --> |X - data train <br> pre-processed|TRfit
    TRfit --> |X - data train <br> transformed|I
    D --> |Y - data train <br> pre-processed|I

  end

  subgraph mod[Modeling]
    J[Train]
  end

  subgraph artefacto[Artefacto de salida]
    TRfit -.-> |pipeline object|TRdb[(Transformer <br> Pipeline)]:::Objclass
    J -.-> |model object| Modeldb[(Train Model <br> Final)]:::Objclass
  end

  I --> |data post-processed|mod
  J -.->N[(Performance <br> Score)]:::Objclass

  N -.-> Scorecheck{{Performance validation <br> Score actual vs anteriores}}:::Checkclass
  Scorecheck -.->  pass{Pass ?}
  pass -.-> |no|no((Alert!)):::Alertclass
  pass -.-> |yes|si(Send Artifact to Deploy):::Passclass
  si -.-> artefacto

  linkStyle 19 stroke:deepskyblue

  classDef Objclass fill:#329cc1;
  classDef Checkclass fill:#EC5800;
  classDef Alertclass fill:#FF0000;
  classDef Passclass fill:#00CC88;
```

## Credits

This project was generated from [@JoseRZapata]'s [data science project template] template.

---
[@JoseRZapata]: https://github.com/JoseRZapata

[data science project template]: https://github.com/JoseRZapata/data-science-project-template

[Data structure]: https://github.com/JoseRZapata/data-science-project-template/blob/main/demo-data-science-template/data/README.md
